{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uYQx_C4sYTsN"
      },
      "source": [
        "\n",
        "## 1. Training and Validation\n",
        "## 2. Testing\n",
        "## 3. Mask to nii format"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LCCbdaAc-L_E"
      },
      "source": [
        "\n",
        "# Patch creation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U7rSJGzF_1fg",
        "outputId": "2514d2eb-03df-436e-ce6c-8d8525d3984f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting patchify\n",
            "  Downloading patchify-0.2.3-py3-none-any.whl (6.6 kB)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.7/dist-packages (from patchify) (1.21.6)\n",
            "Installing collected packages: patchify\n",
            "Successfully installed patchify-0.2.3\n"
          ]
        }
      ],
      "source": [
        "!pip install patchify"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ovHDG7wpkC6Y",
        "outputId": "9a887400-81ba-4585-de96-e5c78d532262"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "For Training :original image 35 - ground truth 35\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 35/35 [00:19<00:00,  1.84it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Augmentation Successful\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import cv2\n",
        "from glob import glob\n",
        "from tqdm import tqdm\n",
        "import imageio\n",
        "from albumentations import CenterCrop\n",
        "from google.colab.patches import cv2_imshow\n",
        "from patchify import patchify\n",
        "\n",
        "# Directory Creation\n",
        "\n",
        "def create_dir(path):\n",
        "    if not os.path.exists(path):\n",
        "        os.makedirs(path)\n",
        "\n",
        "#Load Data\n",
        "\n",
        "def load_data(train_path):\n",
        "    train_x = sorted(glob(os.path.join(train_path,\"image\",\"neo\",\"*.png\")))\n",
        "    train_y = sorted(glob(os.path.join(train_path,\"gt\",\"neo\",\"*.png\")))\n",
        "\n",
        "    return (train_x, train_y)\n",
        "\n",
        "#Augment Data\n",
        "\n",
        "def augment_data(image, gt, save_path, augment=True, format=\"same\"):\n",
        "    size = (1024, 1024)\n",
        "    for idx, (x, y) in tqdm(enumerate(zip(image, gt)), total=len(image)):\n",
        "        \n",
        "        name = x.split(\"/\")[-1].split(\".\")[0]\n",
        "        \n",
        "        #print(name)\n",
        "          \n",
        "        img = cv2.resize(cv2.imread(x, cv2.IMREAD_GRAYSCALE), size)\n",
        "        gt  = cv2.resize(cv2.imread(y, cv2.IMREAD_GRAYSCALE),size )\n",
        "\n",
        "        \n",
        "\n",
        "        if augment == True:\n",
        "            \n",
        "            # Original Image Patch Creation\n",
        "            patches_img = patchify(img,(512,512),step=512) # image patch size = step size means there is no overlap in pathes\n",
        "            \n",
        "            #print(patches_img.shape)#(16,16,64,64): total patch created 16x16=256,of size:(64x64)\n",
        "            #print(patches_img[0].shape)(16,64,64)\n",
        "            \n",
        "            for i in range(patches_img.shape[0]):\n",
        "              for j in range(patches_img.shape[1]):\n",
        "                \n",
        "                  all_patch_img = patches_img[i,j,:,:]\n",
        "                  #cv2_imshow(all_patch_img)\n",
        "                  aug_image_name = f\"{name}_{idx}_{i}_{j}.png\"\n",
        "                  aug_image_path = os.path.join(save_path, \"image\",\"neo\", aug_image_name)\n",
        "                  cv2.imwrite(aug_image_path, all_patch_img)\n",
        "                  #print(f\"{idx}- {i}-{j}\")\n",
        "            # Ground Truth Patch Creation\n",
        "\n",
        "            patches_gt = patchify(gt,(512,512),step=512)\n",
        "            #print(patches_mask.shape)#(16,16,64,64): total patch created 16x16=256,of size:(64x64)\n",
        "            #print(patches_mask[0].shape)\n",
        "\n",
        "            for i in range(patches_gt.shape[0]):\n",
        "              for j in range(patches_gt.shape[1]):\n",
        "                \n",
        "                  all_patch_gt = patches_gt[i,j,:,:]\n",
        "                  #cv2_imshow(all_patch_img)\n",
        "                  aug_gt_name = f\"{name}_{idx}_{i}_{j}.png\"\n",
        "                  aug_gt_path = os.path.join(save_path, \"gt\",\"neo\", aug_gt_name)\n",
        "                  cv2.imwrite(aug_gt_path, all_patch_gt)\n",
        "\n",
        "        \n",
        "\n",
        "        #break\n",
        "        idx+=1\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    np.random.seed(42)\n",
        "\n",
        "    train_data_path = \"/content/drive/MyDrive/Farhana/DRAC22_Dataset/DRAC22_Renamed_3folders/\"\n",
        "    \n",
        "    \"\"\" Load Data \"\"\"\n",
        "    (train_x, train_y) = load_data(train_data_path)\n",
        "\n",
        "    print(f\"For Training :original image {len(train_x)} - ground truth {len(train_y)}\")\n",
        "    \n",
        "\n",
        "    \"\"\" Create directories to save the augmented data \"\"\"\n",
        "    create_dir(\"/content/drive/MyDrive/Farhana/Aug_drac22_data_03/train/image/neo\")\n",
        "    create_dir(\"/content/drive/MyDrive/Farhana/Aug_drac22_data_03/train/gt/neo\")\n",
        "   \n",
        "\n",
        "    \"\"\" Data augmentation \"\"\"\n",
        "    augment_data(train_x, train_y, \"/content/drive/MyDrive/Farhana/Aug_drac22_data_03/train/\", augment=True, format=\"same\")\n",
        "    \n",
        "    print(\"Augmentation Successful\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_VvAKa5OBsiV"
      },
      "source": [
        "\n",
        "\n",
        "#Augmentation"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zIyv5kXWQQr8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mXkrNf1d-oyZ",
        "outputId": "906ef82d-07ce-4e78-b92b-6de2aa717074"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train: 140 - 140\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 140/140 [00:31<00:00,  4.48it/s]\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import cv2\n",
        "from glob import glob\n",
        "from tqdm import tqdm\n",
        "import imageio\n",
        "from albumentations import HorizontalFlip, VerticalFlip, Rotate, GridDistortion, OpticalDistortion, ElasticTransform\n",
        "\n",
        "\n",
        "def create_dir(path):\n",
        "    if not os.path.exists(path):\n",
        "        os.makedirs(path)\n",
        "\n",
        "def load_data(train_path):\n",
        "    train_x = sorted(glob(os.path.join(train_path, \"image\",\"neo\", \"*.png\")))\n",
        "    train_y = sorted(glob(os.path.join(train_path, \"gt\", \"neo\", \"*.png\")))\n",
        "    return (train_x, train_y)\n",
        "\n",
        "def augment_data(images, masks, save_path, augment=True, format=\"same\"):\n",
        "    #size = (512, 512)\n",
        "\n",
        "    for idx, (x, y) in tqdm(enumerate(zip(images, masks)), total=len(images)):\n",
        "        \n",
        "        name = x.split(\"/\")[-1].split(\".\")[0]\n",
        "\n",
        "   \n",
        "        x = cv2.imread(x, cv2.IMREAD_GRAYSCALE)\n",
        "        y = cv2.imread(y, cv2.IMREAD_GRAYSCALE)\n",
        "\n",
        "        if augment == True:\n",
        "            aug = HorizontalFlip(p=1.0)\n",
        "            augmented = aug(image=x, mask=y)\n",
        "            x1 = augmented[\"image\"]\n",
        "            y1 = augmented[\"mask\"]\n",
        "\n",
        "            aug = VerticalFlip(p=1.0)\n",
        "            augmented = aug(image=x, mask=y)\n",
        "            x2 = augmented[\"image\"]\n",
        "            y2 = augmented[\"mask\"]\n",
        "\n",
        "            aug = Rotate(limit=45, p=1.0, border_mode=cv2.BORDER_CONSTANT)\n",
        "            augmented = aug(image=x, mask=y)\n",
        "            x3 = augmented[\"image\"]\n",
        "            y3 = augmented[\"mask\"]\n",
        "            \n",
        "            aug = GridDistortion(p=1)\n",
        "            augmented = aug(image=x, mask=y)\n",
        "            x4 = augmented[\"image\"]\n",
        "            y4 = augmented[\"mask\"]\n",
        "            \n",
        "            aug = OpticalDistortion(p=1, distort_limit=2, shift_limit=0.5, border_mode=cv2.BORDER_CONSTANT)\n",
        "            augmented = aug(image=x, mask=y)\n",
        "            x5 = augmented[\"image\"]\n",
        "            y5 = augmented[\"mask\"]\n",
        "              \n",
        "            aug = ElasticTransform(p=1, alpha=120, sigma=120 * 0.05, alpha_affine=120 * 0.03)\n",
        "            augmented = aug(image=x, mask=y)\n",
        "            x6 = augmented[\"image\"]\n",
        "            y6 = augmented[\"mask\"]\n",
        "\n",
        "\n",
        "            X = [x, x1, x2, x3, x4, x5, x6]  \n",
        "            Y = [y, y1, y2, y3, y4, y5, y6]\n",
        "\n",
        "        else:\n",
        "            X = [x]\n",
        "            Y = [y]\n",
        "\n",
        "        index = 0\n",
        "        for i, m in zip(X, Y):\n",
        "           # i = cv2.resize(i, size)\n",
        "            #m = cv2.resize(m, size)\n",
        "\n",
        "            tmp_image_name = f\"{name}_{index}.png\"\n",
        "            tmp_mask_name = f\"{name}_{index}.png\"\n",
        "\n",
        "            image_path = os.path.join(save_path, \"image\",\"neo\", tmp_image_name)\n",
        "            mask_path = os.path.join(save_path, \"gt\", \"neo\",tmp_mask_name)\n",
        "            #print(\"image\")\n",
        "            #print(image_path)\n",
        "            cv2.imwrite(image_path, i)\n",
        "            cv2.imwrite(mask_path, m)\n",
        "\n",
        "            index += 1\n",
        "        #break\n",
        "        idx+=1    \n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    np.random.seed(42)\n",
        "\n",
        "\n",
        "    train_data_path = \"/content/drive/MyDrive/Farhana/Aug_drac22_data_03/train/\"\n",
        "    \n",
        "    (train_x, train_y) = load_data(train_data_path)\n",
        "\n",
        "    print(f\"Train: {len(train_x)} - {len(train_y)}\")\n",
        "    \n",
        "\n",
        "    \"\"\" Create directories to save the augmented data \"\"\"\n",
        "    create_dir(\"/content/drive/MyDrive/Farhana/Aug_drac22_data_04/train/image/neo\")\n",
        "    create_dir(\"/content/drive/MyDrive/Farhana/Aug_drac22_data_04/train/gt/neo\")\n",
        "    \n",
        "    \"\"\" Data augmentation \"\"\"\n",
        "    augment_data(train_x, train_y, \"/content/drive/MyDrive/Farhana/Aug_drac22_data_04/train/\", augment=True, format=\"same\")\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dksG1A1OBK1x"
      },
      "source": [
        "### Evaluation Metric for Mean Dice and Mean IoU as per DRAC22 Challenge"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RBMBXgbiBLcc",
        "outputId": "1417dfe7-cd36-49dd-9c0c-907c196cafdf"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:31: RuntimeWarning: Mean of empty slice\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:32: RuntimeWarning: Mean of empty slice\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "def get_dice(gt, pred, classId=1):\n",
        "    if np.sum(gt) == 0:\n",
        "        return np.nan\n",
        "    else:\n",
        "        intersection = np.logical_and(gt == classId, pred == classId)\n",
        "        dice_eff = (2. * intersection.sum()) / (gt.sum() + pred.sum())\n",
        "        return dice_eff\n",
        "\n",
        "\n",
        "def get_IoU(gt, pred, classId=1):\n",
        "    if np.sum(gt) == 0:\n",
        "        return np.nan\n",
        "    else:\n",
        "        intersection = np.logical_and(gt == classId, pred == classId)\n",
        "        union = np.logical_or(gt == classId, pred == classId)\n",
        "        iou = np.sum(intersection) / np.sum(union)\n",
        "        return iou\n",
        "\n",
        "\n",
        "def get_mean_IoU_dice(gts_list, preds_list):\n",
        "    assert len(gts_list) == len(preds_list)\n",
        "    dice_list = []\n",
        "    iou_list = []\n",
        "    for gt_array, pred_array in zip(gts_list, preds_list):\n",
        "        dice = get_dice(gt_array, pred_array, 1)\n",
        "        iou = get_IoU(gt_array, pred_array, 1)\n",
        "        dice_list.append(dice)\n",
        "        iou_list.append(iou)\n",
        "    mDice = np.nanmean(dice_list)\n",
        "    mIoU = np.nanmean(iou_list)\n",
        "    return mDice, mIoU\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # gts_list is a list of ground truth mask images\n",
        "    # preds_list is a list of predicted mask images\n",
        "    gt_list = []\n",
        "    pred_list = []\n",
        "    mean_Dice, mean_IoU = get_mean_IoU_dice(gt_list, pred_list)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3xBwYEoZBSb-"
      },
      "source": [
        "### Loss Function : DiceBCE = Dice + BCE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3pNsA3z9BTNB"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class DiceLoss(nn.Module):\n",
        "    def __init__(self, weight=None, size_average=True):\n",
        "        super(DiceLoss, self).__init__()\n",
        "\n",
        "    def forward(self, inputs, targets, smooth=1):\n",
        "\n",
        "        #comment out if your model contains a sigmoid or equivalent activation layer\n",
        "        inputs = torch.sigmoid(inputs)\n",
        "\n",
        "        #flatten label and prediction tensors\n",
        "        inputs = inputs.view(-1)\n",
        "        targets = targets.view(-1)\n",
        "\n",
        "        intersection = (inputs * targets).sum()\n",
        "        dice = (2.*intersection + smooth)/(inputs.sum() + targets.sum() + smooth)\n",
        "\n",
        "        return 1 - dice\n",
        "\n",
        "class FocalLoss(nn.CrossEntropyLoss):\n",
        "    ''' Focal loss for classification tasks on imbalanced datasets '''\n",
        "\n",
        "    def __init__(self, gamma, alpha=None, ignore_index=-100, reduction='none'):\n",
        "        super().__init__(weight=alpha, ignore_index=ignore_index, reduction='none')\n",
        "        self.reduction = reduction\n",
        "        self.gamma = gamma\n",
        "\n",
        "    def forward(self, input_, target):\n",
        "        cross_entropy = super().forward(input_, target)\n",
        "        # Temporarily mask out ignore index to '0' for valid gather-indices input.\n",
        "        # This won't contribute final loss as the cross_entropy contribution\n",
        "        # for these would be zero.\n",
        "        target = target * (target != self.ignore_index).long()\n",
        "        input_prob = torch.gather(F.softmax(input_, 1), 1, target.unsqueeze(1))\n",
        "        loss = torch.pow(1 - input_prob, self.gamma) * cross_entropy\n",
        "        return torch.mean(loss) if self.reduction == 'mean' else torch.sum(loss) if self.reduction == 'sum' else loss\n",
        "\n",
        "class DiceBCELoss(nn.Module):\n",
        "    def __init__(self, weight=None, size_average=True):\n",
        "        super(DiceBCELoss, self).__init__()\n",
        "\n",
        "    def forward(self, inputs, targets, smooth=1):\n",
        "\n",
        "        #comment out if your model contains a sigmoid or equivalent activation layer\n",
        "        inputs = torch.sigmoid(inputs)\n",
        "\n",
        "        #flatten label and prediction tensors\n",
        "        inputs = inputs.view(-1)        \n",
        "        targets = targets.view(-1)\n",
        "\n",
        "        intersection = (inputs * targets).sum()\n",
        "        dice_loss = 1 - (2.*intersection + smooth)/(inputs.sum() + targets.sum() + smooth)\n",
        "        BCE = F.binary_cross_entropy(inputs, targets, reduction='mean')\n",
        "        Dice_BCE = BCE + dice_loss \n",
        "        return Dice_BCE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XuvZ7yN3BlEP"
      },
      "source": [
        "#Utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FtcfpY5tCEbg"
      },
      "outputs": [],
      "source": [
        "#Utils\n",
        "import os\n",
        "import time\n",
        "import random\n",
        "import numpy as np\n",
        "import cv2\n",
        "import torch\n",
        "\n",
        "\"\"\" Seeding the randomness. \"\"\"\n",
        "def seeding(seed):\n",
        "    random.seed(seed)\n",
        "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "\n",
        "\"\"\" Create a directory. \"\"\"\n",
        "def create_dir(path):\n",
        "    if not os.path.exists(path):\n",
        "        os.makedirs(path)\n",
        "\n",
        "\"\"\" Calculate the time taken \"\"\"\n",
        "def epoch_time(start_time, end_time):\n",
        "    elapsed_time = end_time - start_time\n",
        "    elapsed_mins = int(elapsed_time / 60)\n",
        "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
        "    return elapsed_mins, elapsed_secs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZHiyC-c-CwsK"
      },
      "source": [
        "#Making the Data Ready for training (Preprocessing data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HWTbzew3CxQY"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import cv2\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "from PIL import Image\n",
        "from torchvision import transforms\n",
        "import matplotlib.pyplot as plt\n",
        "from numpy import asarray\n",
        "\n",
        "\n",
        "class DriveDataset(Dataset):\n",
        "    def __init__(self, images_path, mask_path, augmentation=None, classes=None):\n",
        "\n",
        "        self.images_path = images_path\n",
        "        self.mask_path = mask_path\n",
        "        self.n_samples = len(images_path)\n",
        "        self.augmentation=augmentation\n",
        "        self.classes=classes\n",
        "        #print(\"12156\") \n",
        "        \n",
        "    def __getitem__(self, index):\n",
        "\n",
        "        size = (512,512)\n",
        "\n",
        "        \"\"\" Reading image \"\"\"\n",
        "        input_image = Image.open(self.images_path[index])\n",
        "        input_image = input_image.convert(\"RGB\") # gray image to RGB image\n",
        "        input_image = asarray(input_image)\n",
        "        input_image = input_image/255.0\n",
        "        input_image = cv2.resize(input_image, size)\n",
        "        preprocess = transforms.Compose([transforms.ToTensor(),transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),]) #Pilimage to Tensor,Normalization in range [0,1]\n",
        "        input_image = preprocess(input_image)\n",
        "\n",
        "        \"\"\" Reading ground truth\"\"\"\n",
        "        input_mask = Image.open(self.mask_path[index]) \n",
        "        input_mask = asarray(input_mask)\n",
        "        input_mask = input_mask/255.0      \n",
        "        input_mask = cv2.resize(input_mask, size)\n",
        "        preprocess2 = transforms.Compose([transforms.ToTensor(),])\n",
        "        input_mask = preprocess2(input_mask)\n",
        "        #plt.imshow(input_mask, cmap='gray')\n",
        "      \n",
        "        return input_image, input_mask\n",
        "        \n",
        "    def __len__(self):\n",
        "        \n",
        "        return self.n_samples\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "  train_x=\"/content/drive/MyDrive/DRAC22_Dataset/DRAC22_Renamed_3folders/image/ima/075_1.png\"\n",
        "  train_y=\"/content/drive/MyDrive/DRAC22_Dataset/DRAC22_Renamed_3folders/gt/ima/075_1.png\"\n",
        "  train_dataset = DriveDataset(train_x, train_y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tDP7GmvxDEWQ"
      },
      "source": [
        "#Pretrained Model: DeepLabv3_ResNet50"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173,
          "referenced_widgets": [
            "80d9fb63d386430cb19f6a2072c58dac",
            "36f49d4a2c534e789b0b89569d9a9341",
            "9f8281320f2d4fc79ad3c16a6a293b22",
            "787ff4d662484357b9765d658d9f380b",
            "54fddd1d3210428a9cfd5cbfd469dd24",
            "1c807654e6ee4329994b34c08b2d1aef",
            "8b45723e2fba45c1b333fc298d4af7ac",
            "c516d91b49e2441782f0b0a5bde14c0b",
            "a5ccf5adf8ea4e26b431762a828d6663",
            "493a3e569a4a4a2f9209065dd5bda90c",
            "abd31cadfa544556a629f92c4ebcf575"
          ]
        },
        "id": "-TQx9EzrDE69",
        "outputId": "b67fa2df-4b36-4cea-b735-a4ddb4329b0c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading: \"https://github.com/pytorch/vision/zipball/v0.10.0\" to /root/.cache/torch/hub/v0.10.0.zip\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/models/_utils.py:209: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n",
            "  f\"The parameter '{pretrained_param}' is deprecated since 0.13 and will be removed in 0.15, \"\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=DeepLabV3_ResNet50_Weights.COCO_WITH_VOC_LABELS_V1`. You can also use `weights=DeepLabV3_ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/deeplabv3_resnet50_coco-cd0a2569.pth\" to /root/.cache/torch/hub/checkpoints/deeplabv3_resnet50_coco-cd0a2569.pth\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "80d9fb63d386430cb19f6a2072c58dac",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0.00/161M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "Model = torch.hub.load('pytorch/vision:v0.10.0', 'deeplabv3_resnet50', pretrained=True)\n",
        "Model.classifier[4]=nn.Conv2d(256, 1, kernel_size=(1, 1), stride=(1, 1)) # changed this layer from outchannel=21 to 1\n",
        "import sys\n",
        "class Identity(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(Identity,self).__init__()\n",
        "\n",
        "  def forward(self,x):\n",
        "    return x\n",
        "Model.aux_classifier=Identity() # Will not be effected by Aux_classifier layer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vetLnkpZDOgt"
      },
      "source": [
        "##Training & Validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XoOAAnt-DHVR",
        "outputId": "0606a70c-4c81-446b-a1db-92514f7522cf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train data loader 221\n",
            "valid data loader 25\n",
            "Number of model parameters: 39633729\n",
            "Training Complete\n",
            "Valid loss improved from inf to 1.1712. Saving checkpoint: /content/drive/MyDrive/Farhana/checkpoint/checkpoint_neo0.pth\n",
            "Epoch: 01 | Epoch Time: 2m 27s\n",
            "\tTrain Loss: 1.334\n",
            "\tVal. Loss: 1.171\n",
            "\tTrain_Mean_Dice: 0.0000 \tTrain: Mean IoU: 0.0000 \n",
            "\tVal_Mean_Dice: 0.0000 \tVal: Mean IoU: 0.0000 \n",
            "Training Complete\n",
            "Valid loss improved from 1.1712 to 1.0922. Saving checkpoint: /content/drive/MyDrive/Farhana/checkpoint/checkpoint_neo0.pth\n",
            "Epoch: 02 | Epoch Time: 1m 52s\n",
            "\tTrain Loss: 1.119\n",
            "\tVal. Loss: 1.092\n",
            "\tTrain_Mean_Dice: 0.0000 \tTrain: Mean IoU: 0.0000 \n",
            "\tVal_Mean_Dice: 0.0000 \tVal: Mean IoU: 0.0000 \n",
            "Training Complete\n",
            "Valid loss improved from 1.0922 to 1.0197. Saving checkpoint: /content/drive/MyDrive/Farhana/checkpoint/checkpoint_neo0.pth\n",
            "Epoch: 03 | Epoch Time: 1m 52s\n",
            "\tTrain Loss: 1.056\n",
            "\tVal. Loss: 1.020\n",
            "\tTrain_Mean_Dice: 0.0158 \tTrain: Mean IoU: 0.0091 \n",
            "\tVal_Mean_Dice: 0.0372 \tVal: Mean IoU: 0.0206 \n",
            "Training Complete\n",
            "Valid loss improved from 1.0197 to 0.9248. Saving checkpoint: /content/drive/MyDrive/Farhana/checkpoint/checkpoint_neo0.pth\n",
            "Epoch: 04 | Epoch Time: 1m 52s\n",
            "\tTrain Loss: 0.949\n",
            "\tVal. Loss: 0.925\n",
            "\tTrain_Mean_Dice: 0.2351 \tTrain: Mean IoU: 0.1491 \n",
            "\tVal_Mean_Dice: 0.2660 \tVal: Mean IoU: 0.1758 \n",
            "Training Complete\n",
            "Valid loss improved from 0.9248 to 0.8560. Saving checkpoint: /content/drive/MyDrive/Farhana/checkpoint/checkpoint_neo0.pth\n",
            "Epoch: 05 | Epoch Time: 1m 52s\n",
            "\tTrain Loss: 0.839\n",
            "\tVal. Loss: 0.856\n",
            "\tTrain_Mean_Dice: 0.3636 \tTrain: Mean IoU: 0.2397 \n",
            "\tVal_Mean_Dice: 0.3284 \tVal: Mean IoU: 0.2101 \n",
            "Training Complete\n",
            "Epoch: 06 | Epoch Time: 1m 52s\n",
            "\tTrain Loss: 0.774\n",
            "\tVal. Loss: 1.149\n",
            "\tTrain_Mean_Dice: 0.3916 \tTrain: Mean IoU: 0.2633 \n",
            "\tVal_Mean_Dice: 0.0090 \tVal: Mean IoU: 0.0047 \n",
            "Training Complete\n",
            "Valid loss improved from 1.1493 to 0.8717. Saving checkpoint: /content/drive/MyDrive/Farhana/checkpoint/checkpoint_neo0.pth\n",
            "Epoch: 07 | Epoch Time: 1m 52s\n",
            "\tTrain Loss: 0.720\n",
            "\tVal. Loss: 0.872\n",
            "\tTrain_Mean_Dice: 0.4371 \tTrain: Mean IoU: 0.2994 \n",
            "\tVal_Mean_Dice: 0.4477 \tVal: Mean IoU: 0.3103 \n",
            "Training Complete\n",
            "Valid loss improved from 0.8717 to 0.7669. Saving checkpoint: /content/drive/MyDrive/Farhana/checkpoint/checkpoint_neo0.pth\n",
            "Epoch: 08 | Epoch Time: 1m 52s\n",
            "\tTrain Loss: 0.698\n",
            "\tVal. Loss: 0.767\n",
            "\tTrain_Mean_Dice: 0.4474 \tTrain: Mean IoU: 0.3110 \n",
            "\tVal_Mean_Dice: 0.4197 \tVal: Mean IoU: 0.2908 \n",
            "Training Complete\n",
            "Valid loss improved from 0.7669 to 0.6160. Saving checkpoint: /content/drive/MyDrive/Farhana/checkpoint/checkpoint_neo0.pth\n",
            "Epoch: 09 | Epoch Time: 1m 52s\n",
            "\tTrain Loss: 0.686\n",
            "\tVal. Loss: 0.616\n",
            "\tTrain_Mean_Dice: 0.4472 \tTrain: Mean IoU: 0.3104 \n",
            "\tVal_Mean_Dice: 0.4847 \tVal: Mean IoU: 0.3492 \n",
            "Training Complete\n",
            "Epoch: 10 | Epoch Time: 1m 52s\n",
            "\tTrain Loss: 0.677\n",
            "\tVal. Loss: 0.709\n",
            "\tTrain_Mean_Dice: 0.4631 \tTrain: Mean IoU: 0.3244 \n",
            "\tVal_Mean_Dice: 0.4170 \tVal: Mean IoU: 0.3053 \n",
            "Training Complete\n",
            "Valid loss improved from 0.7093 to 0.6903. Saving checkpoint: /content/drive/MyDrive/Farhana/checkpoint/checkpoint_neo0.pth\n",
            "Epoch: 11 | Epoch Time: 1m 52s\n",
            "\tTrain Loss: 0.634\n",
            "\tVal. Loss: 0.690\n",
            "\tTrain_Mean_Dice: 0.4802 \tTrain: Mean IoU: 0.3384 \n",
            "\tVal_Mean_Dice: 0.3725 \tVal: Mean IoU: 0.2605 \n",
            "Training Complete\n",
            "Valid loss improved from 0.6903 to 0.5644. Saving checkpoint: /content/drive/MyDrive/Farhana/checkpoint/checkpoint_neo0.pth\n",
            "Epoch: 12 | Epoch Time: 1m 52s\n",
            "\tTrain Loss: 0.647\n",
            "\tVal. Loss: 0.564\n",
            "\tTrain_Mean_Dice: 0.4836 \tTrain: Mean IoU: 0.3413 \n",
            "\tVal_Mean_Dice: 0.5482 \tVal: Mean IoU: 0.4030 \n",
            "Training Complete\n",
            "Epoch: 13 | Epoch Time: 1m 52s\n",
            "\tTrain Loss: 0.632\n",
            "\tVal. Loss: 0.627\n",
            "\tTrain_Mean_Dice: 0.4858 \tTrain: Mean IoU: 0.3461 \n",
            "\tVal_Mean_Dice: 0.4397 \tVal: Mean IoU: 0.3043 \n",
            "Training Complete\n",
            "Valid loss improved from 0.6272 to 0.5902. Saving checkpoint: /content/drive/MyDrive/Farhana/checkpoint/checkpoint_neo0.pth\n",
            "Epoch: 14 | Epoch Time: 1m 52s\n",
            "\tTrain Loss: 0.626\n",
            "\tVal. Loss: 0.590\n",
            "\tTrain_Mean_Dice: 0.4970 \tTrain: Mean IoU: 0.3532 \n",
            "\tVal_Mean_Dice: 0.5085 \tVal: Mean IoU: 0.3787 \n",
            "Training Complete\n",
            "Epoch: 15 | Epoch Time: 1m 52s\n",
            "\tTrain Loss: 0.616\n",
            "\tVal. Loss: 0.654\n",
            "\tTrain_Mean_Dice: 0.4965 \tTrain: Mean IoU: 0.3535 \n",
            "\tVal_Mean_Dice: 0.4806 \tVal: Mean IoU: 0.3462 \n",
            "Training Complete\n",
            "Valid loss improved from 0.6536 to 0.6026. Saving checkpoint: /content/drive/MyDrive/Farhana/checkpoint/checkpoint_neo0.pth\n",
            "Epoch: 16 | Epoch Time: 1m 52s\n",
            "\tTrain Loss: 0.631\n",
            "\tVal. Loss: 0.603\n",
            "\tTrain_Mean_Dice: 0.4977 \tTrain: Mean IoU: 0.3521 \n",
            "\tVal_Mean_Dice: 0.4721 \tVal: Mean IoU: 0.3485 \n",
            "Training Complete\n",
            "Epoch: 17 | Epoch Time: 1m 52s\n",
            "\tTrain Loss: 0.635\n",
            "\tVal. Loss: 0.689\n",
            "\tTrain_Mean_Dice: 0.4870 \tTrain: Mean IoU: 0.3452 \n",
            "\tVal_Mean_Dice: 0.4293 \tVal: Mean IoU: 0.3139 \n",
            "Training Complete\n",
            "Valid loss improved from 0.6885 to 0.6002. Saving checkpoint: /content/drive/MyDrive/Farhana/checkpoint/checkpoint_neo0.pth\n",
            "Epoch: 18 | Epoch Time: 1m 52s\n",
            "\tTrain Loss: 0.636\n",
            "\tVal. Loss: 0.600\n",
            "\tTrain_Mean_Dice: 0.4932 \tTrain: Mean IoU: 0.3520 \n",
            "\tVal_Mean_Dice: 0.5349 \tVal: Mean IoU: 0.3790 \n",
            "Training Complete\n",
            "Epoch: 19 | Epoch Time: 1m 52s\n",
            "\tTrain Loss: 0.621\n",
            "\tVal. Loss: 0.692\n",
            "\tTrain_Mean_Dice: 0.4969 \tTrain: Mean IoU: 0.3553 \n",
            "\tVal_Mean_Dice: 0.4626 \tVal: Mean IoU: 0.3310 \n",
            "Training Complete\n",
            "Valid loss improved from 0.6915 to 0.6894. Saving checkpoint: /content/drive/MyDrive/Farhana/checkpoint/checkpoint_neo0.pth\n",
            "Epoch: 20 | Epoch Time: 1m 52s\n",
            "\tTrain Loss: 0.626\n",
            "\tVal. Loss: 0.689\n",
            "\tTrain_Mean_Dice: 0.4956 \tTrain: Mean IoU: 0.3541 \n",
            "\tVal_Mean_Dice: 0.4152 \tVal: Mean IoU: 0.3135 \n",
            "Training Complete\n",
            "Valid loss improved from 0.6894 to 0.6242. Saving checkpoint: /content/drive/MyDrive/Farhana/checkpoint/checkpoint_neo0.pth\n",
            "Epoch: 21 | Epoch Time: 1m 52s\n",
            "\tTrain Loss: 0.642\n",
            "\tVal. Loss: 0.624\n",
            "\tTrain_Mean_Dice: 0.4760 \tTrain: Mean IoU: 0.3371 \n",
            "\tVal_Mean_Dice: 0.4541 \tVal: Mean IoU: 0.3232 \n",
            "Training Complete\n",
            "Epoch: 22 | Epoch Time: 1m 52s\n",
            "\tTrain Loss: 0.646\n",
            "\tVal. Loss: 0.743\n",
            "\tTrain_Mean_Dice: 0.4781 \tTrain: Mean IoU: 0.3379 \n",
            "\tVal_Mean_Dice: 0.3644 \tVal: Mean IoU: 0.2594 \n",
            "Training Complete\n",
            "Epoch: 23 | Epoch Time: 1m 52s\n",
            "\tTrain Loss: 0.627\n",
            "\tVal. Loss: 0.786\n",
            "\tTrain_Mean_Dice: 0.4958 \tTrain: Mean IoU: 0.3519 \n",
            "\tVal_Mean_Dice: 0.2733 \tVal: Mean IoU: 0.1816 \n",
            "Training Complete\n",
            "Valid loss improved from 0.7865 to 0.7091. Saving checkpoint: /content/drive/MyDrive/Farhana/checkpoint/checkpoint_neo0.pth\n",
            "Epoch: 24 | Epoch Time: 1m 52s\n",
            "\tTrain Loss: 0.631\n",
            "\tVal. Loss: 0.709\n",
            "\tTrain_Mean_Dice: 0.4907 \tTrain: Mean IoU: 0.3498 \n",
            "\tVal_Mean_Dice: 0.3877 \tVal: Mean IoU: 0.2729 \n",
            "Training Complete\n",
            "Valid loss improved from 0.7091 to 0.6497. Saving checkpoint: /content/drive/MyDrive/Farhana/checkpoint/checkpoint_neo0.pth\n",
            "Epoch: 25 | Epoch Time: 1m 52s\n",
            "\tTrain Loss: 0.666\n",
            "\tVal. Loss: 0.650\n",
            "\tTrain_Mean_Dice: 0.4578 \tTrain: Mean IoU: 0.3211 \n",
            "\tVal_Mean_Dice: 0.5548 \tVal: Mean IoU: 0.4039 \n",
            "Training Complete\n",
            "Epoch: 26 | Epoch Time: 1m 52s\n",
            "\tTrain Loss: 0.673\n",
            "\tVal. Loss: 0.710\n",
            "\tTrain_Mean_Dice: 0.4535 \tTrain: Mean IoU: 0.3183 \n",
            "\tVal_Mean_Dice: 0.4772 \tVal: Mean IoU: 0.3393 \n",
            "Training Complete\n",
            "Epoch: 27 | Epoch Time: 1m 52s\n",
            "\tTrain Loss: 0.655\n",
            "\tVal. Loss: 0.731\n",
            "\tTrain_Mean_Dice: 0.4649 \tTrain: Mean IoU: 0.3226 \n",
            "\tVal_Mean_Dice: 0.4685 \tVal: Mean IoU: 0.3427 \n",
            "Training Complete\n",
            "Valid loss improved from 0.7311 to 0.6890. Saving checkpoint: /content/drive/MyDrive/Farhana/checkpoint/checkpoint_neo0.pth\n",
            "Epoch: 28 | Epoch Time: 1m 52s\n",
            "\tTrain Loss: 0.651\n",
            "\tVal. Loss: 0.689\n",
            "\tTrain_Mean_Dice: 0.4737 \tTrain: Mean IoU: 0.3385 \n",
            "\tVal_Mean_Dice: 0.3879 \tVal: Mean IoU: 0.2665 \n",
            "Training Complete\n",
            "Epoch: 29 | Epoch Time: 1m 52s\n",
            "\tTrain Loss: 0.645\n",
            "\tVal. Loss: 0.816\n",
            "\tTrain_Mean_Dice: 0.4818 \tTrain: Mean IoU: 0.3395 \n",
            "\tVal_Mean_Dice: 0.5150 \tVal: Mean IoU: 0.3731 \n",
            "Training Complete\n",
            "Valid loss improved from 0.8160 to 0.7910. Saving checkpoint: /content/drive/MyDrive/Farhana/checkpoint/checkpoint_neo0.pth\n",
            "Epoch: 30 | Epoch Time: 1m 52s\n",
            "\tTrain Loss: 0.628\n",
            "\tVal. Loss: 0.791\n",
            "\tTrain_Mean_Dice: 0.4997 \tTrain: Mean IoU: 0.3551 \n",
            "\tVal_Mean_Dice: 0.4695 \tVal: Mean IoU: 0.3451 \n",
            "Training Complete\n",
            "Valid loss improved from 0.7910 to 0.6552. Saving checkpoint: /content/drive/MyDrive/Farhana/checkpoint/checkpoint_neo0.pth\n",
            "Epoch: 31 | Epoch Time: 1m 52s\n",
            "\tTrain Loss: 0.644\n",
            "\tVal. Loss: 0.655\n",
            "\tTrain_Mean_Dice: 0.4690 \tTrain: Mean IoU: 0.3330 \n",
            "\tVal_Mean_Dice: 0.4093 \tVal: Mean IoU: 0.2958 \n",
            "Training Complete\n",
            "Epoch: 32 | Epoch Time: 1m 52s\n",
            "\tTrain Loss: 0.669\n",
            "\tVal. Loss: 0.852\n",
            "\tTrain_Mean_Dice: 0.4616 \tTrain: Mean IoU: 0.3258 \n",
            "\tVal_Mean_Dice: 0.2798 \tVal: Mean IoU: 0.1977 \n",
            "Training Complete\n",
            "Epoch: 33 | Epoch Time: 1m 52s\n",
            "\tTrain Loss: 0.700\n",
            "\tVal. Loss: 0.985\n",
            "\tTrain_Mean_Dice: 0.4288 \tTrain: Mean IoU: 0.2994 \n",
            "\tVal_Mean_Dice: 0.0965 \tVal: Mean IoU: 0.0560 \n",
            "Training Complete\n",
            "Valid loss improved from 0.9851 to 0.7347. Saving checkpoint: /content/drive/MyDrive/Farhana/checkpoint/checkpoint_neo0.pth\n",
            "Epoch: 34 | Epoch Time: 1m 51s\n",
            "\tTrain Loss: 0.647\n",
            "\tVal. Loss: 0.735\n",
            "\tTrain_Mean_Dice: 0.4762 \tTrain: Mean IoU: 0.3375 \n",
            "\tVal_Mean_Dice: 0.3842 \tVal: Mean IoU: 0.2767 \n",
            "Training Complete\n",
            "Valid loss improved from 0.7347 to 0.6351. Saving checkpoint: /content/drive/MyDrive/Farhana/checkpoint/checkpoint_neo0.pth\n",
            "Epoch: 35 | Epoch Time: 1m 51s\n",
            "\tTrain Loss: 0.663\n",
            "\tVal. Loss: 0.635\n",
            "\tTrain_Mean_Dice: 0.4591 \tTrain: Mean IoU: 0.3251 \n",
            "\tVal_Mean_Dice: 0.5595 \tVal: Mean IoU: 0.4051 \n",
            "Training Complete\n",
            "Epoch: 36 | Epoch Time: 1m 51s\n",
            "\tTrain Loss: 0.692\n",
            "\tVal. Loss: 0.688\n",
            "\tTrain_Mean_Dice: 0.4473 \tTrain: Mean IoU: 0.3121 \n",
            "\tVal_Mean_Dice: 0.4254 \tVal: Mean IoU: 0.2993 \n",
            "Training Complete\n",
            "Epoch: 37 | Epoch Time: 1m 52s\n",
            "\tTrain Loss: 0.683\n",
            "\tVal. Loss: 0.715\n",
            "\tTrain_Mean_Dice: 0.4383 \tTrain: Mean IoU: 0.3102 \n",
            "\tVal_Mean_Dice: 0.4246 \tVal: Mean IoU: 0.3017 \n",
            "Training Complete\n",
            "Epoch: 38 | Epoch Time: 1m 52s\n",
            "\tTrain Loss: 0.736\n",
            "\tVal. Loss: 0.970\n",
            "\tTrain_Mean_Dice: 0.3889 \tTrain: Mean IoU: 0.2698 \n",
            "\tVal_Mean_Dice: 0.2134 \tVal: Mean IoU: 0.1326 \n",
            "Training Complete\n",
            "Valid loss improved from 0.9705 to 0.7589. Saving checkpoint: /content/drive/MyDrive/Farhana/checkpoint/checkpoint_neo0.pth\n",
            "Epoch: 39 | Epoch Time: 1m 52s\n",
            "\tTrain Loss: 0.724\n",
            "\tVal. Loss: 0.759\n",
            "\tTrain_Mean_Dice: 0.3883 \tTrain: Mean IoU: 0.2667 \n",
            "\tVal_Mean_Dice: 0.4311 \tVal: Mean IoU: 0.2918 \n",
            "Training Complete\n",
            "Valid loss improved from 0.7589 to 0.6483. Saving checkpoint: /content/drive/MyDrive/Farhana/checkpoint/checkpoint_neo0.pth\n",
            "Epoch: 40 | Epoch Time: 1m 52s\n",
            "\tTrain Loss: 0.703\n",
            "\tVal. Loss: 0.648\n",
            "\tTrain_Mean_Dice: 0.4211 \tTrain: Mean IoU: 0.2888 \n",
            "\tVal_Mean_Dice: 0.4938 \tVal: Mean IoU: 0.3613 \n",
            "Training Complete\n",
            "Epoch: 41 | Epoch Time: 1m 52s\n",
            "\tTrain Loss: 0.706\n",
            "\tVal. Loss: 0.674\n",
            "\tTrain_Mean_Dice: 0.4042 \tTrain: Mean IoU: 0.2796 \n",
            "\tVal_Mean_Dice: 0.4124 \tVal: Mean IoU: 0.2870 \n",
            "Training Complete\n",
            "Epoch: 42 | Epoch Time: 1m 52s\n",
            "\tTrain Loss: 0.722\n",
            "\tVal. Loss: 0.792\n",
            "\tTrain_Mean_Dice: 0.3974 \tTrain: Mean IoU: 0.2731 \n",
            "\tVal_Mean_Dice: 0.3652 \tVal: Mean IoU: 0.2499 \n",
            "Training Complete\n",
            "Epoch: 43 | Epoch Time: 1m 52s\n",
            "\tTrain Loss: 0.759\n",
            "\tVal. Loss: 0.899\n",
            "\tTrain_Mean_Dice: 0.3662 \tTrain: Mean IoU: 0.2485 \n",
            "\tVal_Mean_Dice: 0.2009 \tVal: Mean IoU: 0.1191 \n",
            "Training Complete\n",
            "Valid loss improved from 0.8992 to 0.8115. Saving checkpoint: /content/drive/MyDrive/Farhana/checkpoint/checkpoint_neo0.pth\n",
            "Epoch: 44 | Epoch Time: 1m 52s\n",
            "\tTrain Loss: 0.745\n",
            "\tVal. Loss: 0.811\n",
            "\tTrain_Mean_Dice: 0.3731 \tTrain: Mean IoU: 0.2549 \n",
            "\tVal_Mean_Dice: 0.2999 \tVal: Mean IoU: 0.2060 \n",
            "Training Complete\n",
            "Valid loss improved from 0.8115 to 0.7950. Saving checkpoint: /content/drive/MyDrive/Farhana/checkpoint/checkpoint_neo0.pth\n",
            "Epoch: 45 | Epoch Time: 1m 52s\n",
            "\tTrain Loss: 0.754\n",
            "\tVal. Loss: 0.795\n",
            "\tTrain_Mean_Dice: 0.3653 \tTrain: Mean IoU: 0.2472 \n",
            "\tVal_Mean_Dice: 0.3225 \tVal: Mean IoU: 0.2330 \n",
            "Training Complete\n",
            "Epoch: 46 | Epoch Time: 1m 52s\n",
            "\tTrain Loss: 0.772\n",
            "\tVal. Loss: 0.922\n",
            "\tTrain_Mean_Dice: 0.3448 \tTrain: Mean IoU: 0.2334 \n",
            "\tVal_Mean_Dice: 0.2370 \tVal: Mean IoU: 0.1550 \n",
            "Training Complete\n",
            "Valid loss improved from 0.9220 to 0.7828. Saving checkpoint: /content/drive/MyDrive/Farhana/checkpoint/checkpoint_neo0.pth\n",
            "Epoch: 47 | Epoch Time: 1m 52s\n",
            "\tTrain Loss: 0.758\n",
            "\tVal. Loss: 0.783\n",
            "\tTrain_Mean_Dice: 0.3606 \tTrain: Mean IoU: 0.2447 \n",
            "\tVal_Mean_Dice: 0.3254 \tVal: Mean IoU: 0.2268 \n",
            "Training Complete\n",
            "Epoch: 48 | Epoch Time: 1m 52s\n",
            "\tTrain Loss: 0.782\n",
            "\tVal. Loss: 0.909\n",
            "\tTrain_Mean_Dice: 0.3452 \tTrain: Mean IoU: 0.2291 \n",
            "\tVal_Mean_Dice: 0.2145 \tVal: Mean IoU: 0.1351 \n",
            "Training Complete\n",
            "Valid loss improved from 0.9091 to 0.8650. Saving checkpoint: /content/drive/MyDrive/Farhana/checkpoint/checkpoint_neo0.pth\n",
            "Epoch: 49 | Epoch Time: 1m 51s\n",
            "\tTrain Loss: 0.825\n",
            "\tVal. Loss: 0.865\n",
            "\tTrain_Mean_Dice: 0.3063 \tTrain: Mean IoU: 0.2017 \n",
            "\tVal_Mean_Dice: 0.2400 \tVal: Mean IoU: 0.1494 \n",
            "Training Complete\n",
            "Epoch: 50 | Epoch Time: 1m 52s\n",
            "\tTrain Loss: 0.869\n",
            "\tVal. Loss: 0.908\n",
            "\tTrain_Mean_Dice: 0.2459 \tTrain: Mean IoU: 0.1614 \n",
            "\tVal_Mean_Dice: 0.2309 \tVal: Mean IoU: 0.1483 \n",
            "\n",
            "Training complete\n",
            "Best train Dice:0.4997\t Best train IoU:0.3553\n",
            "Best Val Dice:0.5595\tBest Val IoU:0.4051\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import time\n",
        "from glob import glob\n",
        "from operator import add, sub\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import KFold\n",
        "import random\n",
        "import csv\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader,TensorDataset,random_split,SubsetRandomSampler, ConcatDataset\n",
        "import torch.nn as nn\n",
        "import gc\n",
        "#from pytorchtools import EarlyStopping\n",
        "\n",
        "\n",
        "# This function lodes the training dataset and divide it into two non-overlapping validation and traning datasets\n",
        "\n",
        "def get_train_valid_loader(data_dir,\n",
        "                           batch_size,\n",
        "                           random_seed,\n",
        "                           kfolde=10,\n",
        "                           shuffle=True,\n",
        "                           show_sample=False,\n",
        "                           num_workers=1,\n",
        "                           pin_memory=False,\n",
        "                           shuffelthevaluditation=1): \n",
        "\n",
        "    # load the dataset\n",
        "    train_x = sorted(glob(os.path.join(data_dir,  \"image\",\"neo\", \"*\")))[:882]\n",
        "    train_y = sorted(glob(os.path.join(data_dir, \"gt\",\"neo\", \"*\")))[:882]\n",
        "\n",
        "    valid_x = sorted(glob(os.path.join(data_dir, \"image\",\"neo\", \"*\")))[882:]\n",
        "    valid_y = sorted(glob(os.path.join(data_dir, \"gt\",\"neo\", \"*\")))[882:]\n",
        "    \n",
        "    \"\"\"\n",
        "    print(\"train image\",len(train_x))\n",
        "    print(\"train mask\",len(train_y))\n",
        "    print(\"valid image\",len(valid_x)) \n",
        "    print(\"valid mask\",len(valid_y)) \n",
        "\n",
        "    \"\"\"\n",
        "    train_dataset = DriveDataset(train_x, train_y)\n",
        "    valid_dataset = DriveDataset(valid_x, valid_y)\n",
        "   \n",
        "    dataset=ConcatDataset([train_dataset, valid_dataset])\n",
        "    \n",
        "    valid_size=(len(dataset)/kfolde)/(len(dataset))\n",
        "    num_train = len(dataset)\n",
        "    indices = list(range(num_train))\n",
        "    split = int(np.floor(valid_size * num_train))\n",
        "    \n",
        "    if shuffle:\n",
        "        np.random.seed(random_seed)\n",
        "        np.random.shuffle(indices)\n",
        "\n",
        "    train_idx, valid_idx = indices[:split* (int( shuffelthevaluditation) -1 )] + indices[split*int( shuffelthevaluditation):], indices[split * (int( shuffelthevaluditation) -1 ):split * int( shuffelthevaluditation)]\n",
        "    train_sampler = SubsetRandomSampler(train_idx)\n",
        "    valid_sampler = SubsetRandomSampler(valid_idx)\n",
        "\n",
        "    data_str = f\"Dataset Size:\\nTrain sampler: {len(train_sampler)} - Validsampler: {len(valid_sampler)}\\n\"\n",
        "    \n",
        "    train_loader = torch.utils.data.DataLoader(\n",
        "        dataset, batch_size=batch_size, sampler=train_sampler,\n",
        "        num_workers=num_workers, pin_memory=pin_memory,\n",
        "    )\n",
        "    \n",
        "    valid_loader = torch.utils.data.DataLoader(\n",
        "        dataset, batch_size=batch_size, sampler=valid_sampler,\n",
        "        num_workers=num_workers, pin_memory=pin_memory,\n",
        "    ) \n",
        "    \n",
        "    return (train_loader, valid_loader)\n",
        "\n",
        "def tensor_to_binary(y_true, y_pred):\n",
        "    \"\"\"Ground Truth: Transfering From GPU to CPU and from tensor to numpy \"\"\"\n",
        "    y_true = y_true.detach().cpu().numpy()\n",
        "    y_true = y_true > 0.5\n",
        "    y_true = y_true.astype(np.uint8)\n",
        "    y_true = y_true.reshape(-1)\n",
        "\n",
        "    \"\"\" Prediction: Transfering From GPU to CPU and from tensor to numpy  \"\"\"\n",
        "\n",
        "    y_pred = y_pred.detach().cpu().numpy()\n",
        "    y_pred = y_pred > 0.5\n",
        "    y_pred = y_pred.astype(np.uint8)\n",
        "    y_pred = y_pred.reshape(-1)\n",
        "\n",
        "    return y_true, y_pred\n",
        "\n",
        "def train(model, loader, optimizer, loss_fn, device):\n",
        "    epoch_loss = 0.0\n",
        "    gt_list = []\n",
        "    pred_list = []\n",
        "   \n",
        "    model.train()\n",
        "    for x, y in loader:\n",
        "        x = x.to(device, dtype=torch.float32)\n",
        "        y = y.to(device, dtype=torch.float32)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        y_pred = model(x)['out']\n",
        "                     \n",
        "        loss = loss_fn(y_pred, y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        epoch_loss += float(loss.item())\n",
        "        \n",
        "        y, y_pred=tensor_to_binary(y, y_pred) \n",
        "        gt_list.append(y)\n",
        "        pred_list.append(y_pred)\n",
        "    \n",
        "    epoch_loss = epoch_loss/len(loader)\n",
        "    mean_Dice, mean_IoU = get_mean_IoU_dice(gt_list, pred_list)\n",
        "    print(\"Training Complete\")\n",
        "    return epoch_loss, mean_Dice, mean_IoU\n",
        "        \n",
        "def evaluate(model, loader, loss_fn, device):\n",
        "    epoch_loss = 0.0\n",
        "    gt_list = []\n",
        "    pred_list = []\n",
        "   \n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for x, y in loader:\n",
        "            x = x.to(device, dtype=torch.float32)\n",
        "            y = y.to(device, dtype=torch.float32)\n",
        "\n",
        "            y_pred = model(x)['out']\n",
        "            loss = loss_fn(y_pred, y)\n",
        "            epoch_loss += float(loss.item())\n",
        "                       \n",
        "            y, y_pred=tensor_to_binary(y, y_pred) \n",
        "            gt_list.append(y)\n",
        "            pred_list.append(y_pred)\n",
        "\n",
        "    epoch_loss = epoch_loss/len(loader)\n",
        "    mean_Dice, mean_IoU = get_mean_IoU_dice(gt_list, pred_list)\n",
        "    return epoch_loss, mean_Dice, mean_IoU \n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    \"\"\" Seeding \"\"\"\n",
        "    seeding(42)\n",
        "\n",
        "    \"\"\" Directories \"\"\"\n",
        "    \n",
        "    create_dir(\"/content/drive/MyDrive/Farhana/checkpoint/\")\n",
        "    \n",
        "    \"\"\" Load test dataset \"\"\"\n",
        "    data_dir=\"/content/drive/MyDrive/Farhana/Aug_drac22_data_04/train/\"\n",
        "\n",
        "    #early_stopping = EarlyStopping(patience=patience, verbose=True)\n",
        "    \n",
        "    \n",
        "    \"\"\" Hyperparameters \"\"\"\n",
        "    num_epochs=50\n",
        "    batch_size=4\n",
        "    k=10\n",
        "    lr = 1e-4\n",
        "    \n",
        "    #checkpoint_path= \"/content/drive/MyDrive/Farhana/checkpoint/checkpoint_ima0.pth\"\n",
        "    #checkpoint_path= \"/content/drive/MyDrive/Farhana/checkpoint/checkpoint_npa0.pth\"\n",
        "    checkpoint_path= \"/content/drive/MyDrive/Farhana/checkpoint/checkpoint_neo0.pth\"\n",
        "\n",
        "    #heckpoint_path= \"/content/drive/MyDrive/Farhana/checkpoint/checkpoint_ima2.pth\"\n",
        "    #checkpoint_path= \"/content/drive/MyDrive/Farhana/checkpoint/checkpoint_neo2.pth\"\n",
        "    #checkpoint_path= \"/content/drive/MyDrive/Farhana/checkpoint/checkpoint_npa2.pth\"\n",
        "\n",
        "\n",
        "    #checkpoint_path= \"/content/drive/MyDrive/Farhana/Result/checkpoint_ima.pth\"\n",
        "    #checkpoint_path= \"/content/drive/MyDrive/Farhana/checkpoint/checkpoint_npa.pth\"\n",
        "    #checkpoint_path= \"/content/drive/MyDrive/Farhana/checkpoint/checkpoint_neo.pth\"\n",
        "\n",
        "    \"\"\" Dataset and loader \"\"\"\n",
        "    \n",
        "    train_loader, valid_loader=get_train_valid_loader(data_dir=data_dir,\n",
        "                                                      batch_size=batch_size,\n",
        "                                                      random_seed=False,\n",
        "                                                      kfolde=k,\n",
        "                                                      shuffle=True,\n",
        "                                                      show_sample=False,\n",
        "                                                      num_workers=2,\n",
        "                                                      pin_memory=False,\n",
        "                                                      shuffelthevaluditation=1)\n",
        "    print(\"train data loader\",len(train_loader))\n",
        "    print(\"valid data loader\",len(valid_loader)) \n",
        "    \n",
        "    \n",
        "    #loaded_checkpoint=torch.load(checkpoint_path)\n",
        "    \n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    model = Model\n",
        "    model = model.to(device)\n",
        "    print('Number of model parameters: {}'.format(\n",
        "      sum([p.data.nelement() for p in model.parameters()])))  \n",
        "        \n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=5, verbose=True)\n",
        "\n",
        "    #model.load_state_dict(loaded_checkpoint) #loading the trained model\n",
        "\n",
        "    loss_fn = DiceBCELoss().to(device)\n",
        "      \n",
        "    with open('/content/drive/MyDrive/Farhana/drac22_tarin_val_result/submission/train_details_neo_0.csv','a+') as td:\n",
        "        training_details=csv.writer(td)\n",
        "        with open('/content/drive/MyDrive/Farhana/drac22_tarin_val_result/submission/val_details_neo_0.csv','a+') as vd:\n",
        "            val_details=csv.writer(vd)\n",
        "\n",
        "            \"\"\" Training the model \"\"\"\n",
        "            best_valid_loss = float(\"inf\")  \n",
        "            best_tr_dice = 0.000\n",
        "            best_tr_iou = 0.000\n",
        "            best_val_dice = 0.000\n",
        "            best_val_iou = 0.000   \n",
        "\n",
        "            # Early stopping\n",
        "            patience = 5\n",
        "            trigger_times = 0\n",
        "            \n",
        "\n",
        "            for epoch in range(num_epochs):\n",
        "              start_time = time.time()\n",
        "              train_loss, tr_mean_dice, tr_mean_iou = train(model, train_loader, optimizer, loss_fn, device)\n",
        "              valid_loss, val_mean_dice, val_mean_iou = evaluate(model, valid_loader, loss_fn, device)\n",
        "\n",
        "              \"\"\" Saving the model \"\"\"\n",
        "              if valid_loss < best_valid_loss:\n",
        "                  data_str = f\"Valid loss improved from {best_valid_loss:2.4f} to {valid_loss:2.4f}. Saving checkpoint: {checkpoint_path}\"\n",
        "                  print(data_str)\n",
        "              #else:\n",
        "                 # trigger_times += 1\n",
        "                 # print('Trigger Times:', trigger_times)\n",
        "\n",
        "              best_valid_loss = valid_loss\n",
        "              torch.save(model.state_dict(), checkpoint_path)\n",
        "\n",
        "              if tr_mean_dice >  best_tr_dice:\n",
        "                 best_tr_dice = tr_mean_dice\n",
        "\n",
        "              if tr_mean_iou >  best_tr_iou:\n",
        "                 best_tr_iou = tr_mean_iou\n",
        "\n",
        "              if val_mean_dice > best_val_dice:\n",
        "                 best_val_dice = val_mean_dice\n",
        "\n",
        "              if val_mean_iou > best_val_iou:\n",
        "                 best_val_iou = val_mean_iou\n",
        "\n",
        "              if trigger_times >= patience:\n",
        "                print('Early stopping!\\nStart to test process.')\n",
        "                #break\n",
        "\n",
        "\n",
        "              end_time = time.time()\n",
        "              epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "\n",
        "              data_str = f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s\\n'\n",
        "              data_str += f'\\tTrain Loss: {train_loss:.3f}\\n'\n",
        "              data_str += f'\\tVal. Loss: {valid_loss:.3f}'\n",
        "              print(data_str)\n",
        "\n",
        "              print(f\"\\tTrain_Mean_Dice: {tr_mean_dice:1.4f} \\tTrain: Mean IoU: {tr_mean_iou:1.4f} \")\n",
        "              print(f\"\\tVal_Mean_Dice: {val_mean_dice:1.4f} \\tVal: Mean IoU: {val_mean_iou:1.4f} \") \n",
        "\n",
        "              training_details.writerow([str(epoch+1),float(train_loss),float(tr_mean_dice),float(tr_mean_iou)])                    \n",
        "              val_details.writerow([str(epoch+1),float(valid_loss),float(val_mean_dice),float(val_mean_iou)])                                  \n",
        "                    \n",
        "              train_loader, valid_loader = get_train_valid_loader(kfolde=k,shuffelthevaluditation=(epoch%k)+1,random_seed=False,data_dir=data_dir,batch_size=batch_size, shuffle=True)\n",
        "              \n",
        "              gc.collect()\n",
        "              \n",
        "              torch.cuda.empty_cache()  \n",
        "\n",
        "    print(\"\\nTraining complete\")\n",
        "    print(f\"Best train Dice:{best_tr_dice:1.4f}\\t Best train IoU:{best_tr_iou:1.4f}\")\n",
        "    print(f\"Best Val Dice:{best_val_dice:1.4f}\\tBest Val IoU:{best_val_iou:1.4f}\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    \n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4F4q3eSJqtJs"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jOE9ckXrValM"
      },
      "source": [
        "#Test the Model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TWBTmWGKcFl9",
        "outputId": "45613d28-a6b8-4947-9459-7b25b1e87ca9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 65/65 [00:08<00:00,  8.02it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Prediction Completed\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "from tqdm import tqdm\n",
        "import cv2\n",
        "import os\n",
        "import time\n",
        "from glob import glob\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "from PIL import Image\n",
        "from torchvision import transforms\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def create_dir(path):\n",
        "    if not os.path.exists(path):\n",
        "        os.makedirs(path)\n",
        "\n",
        "def predict_data( test_image_path,save_path):\n",
        "   \n",
        "    #print(test_image_path)\n",
        "    #print(save_path)\n",
        "    for idx, x in tqdm(enumerate(test_image_path), total=len(test_image_path)):\n",
        "        \n",
        "        name = x.split(\"/\")[-1].split(\".\")[0]\n",
        "        #print(name)\n",
        "        \n",
        "        input_image = Image.open(x)\n",
        "        input_image = input_image.convert(\"RGB\")\n",
        "        input_image = asarray(input_image)\n",
        "        input_image = input_image/255.0\n",
        "        preprocess  = transforms.Compose([transforms.ToTensor(),transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),])\n",
        "        input_image = preprocess(input_image)\n",
        "        input_image = input_image.to(device)\n",
        "        input_image = torch.unsqueeze(input_image, 0)\n",
        "        input_image = input_image.float()\n",
        "        \n",
        "\n",
        "\n",
        "        \n",
        "        with torch.no_grad():\n",
        "          \"\"\" Prediction  \"\"\"\n",
        "          pred_y = model(input_image)[\"out\"]\n",
        "          pred_y = torch.sigmoid(pred_y)\n",
        "          pred_y = pred_y[0].cpu().numpy()   ##(1,512,512)     \n",
        "          pred_y = np.squeeze(pred_y, axis=0) ## (512,512)   \n",
        "          pred_y = pred_y > 0.5\n",
        "          pred_y = np.array(pred_y, dtype=np.uint8)##tensor to nparray conversion\n",
        "          \n",
        "\n",
        "        \"\"\" Saving masks \"\"\"\n",
        "        tmp_pred_name = f\"{name}.png\"\n",
        "        pred_path = os.path.join(save_path, tmp_pred_name)\n",
        "       # print(F\"PRED PATH:{pred_path}\")\n",
        "        cv2.imwrite(pred_path, pred_y)\n",
        "        #break\n",
        "        idx+=1\n",
        "\n",
        "\n",
        "if __name__==\"__main__\":\n",
        "  \"\"\"Seeding\"\"\"\n",
        "  seeding(42)\n",
        "\n",
        "  \"\"\"Folders\"\"\"\n",
        "  #to save the prediction mask\n",
        "  #create_dir(\"/content/drive/MyDrive/Farhana/DRAC2022_Prediction2/IMA0\") \n",
        "  #create_dir(\"/content/drive/MyDrive/Farhana/DRAC2022_Prediction2/NPA0\") \n",
        "  create_dir(\"/content/drive/MyDrive/Farhana/DRAC2022_Prediction2/NEO0\") \n",
        "\n",
        "  \"\"\"Load Data\"\"\"\n",
        "  test_image_path = sorted(glob(\"/content/drive/MyDrive/Farhana/DRAC2022_Testing_Set/A. Segmentation/1. Original Images/b. Testing Set/*\"))\n",
        "\n",
        "  #predicted_save_path=\"/content/drive/MyDrive/Farhana/DRAC2022_Prediction2/IMA0/\"\n",
        "  #predicted_save_path=\"/content/drive/MyDrive/Farhana/DRAC2022_Prediction2/NPA0/\" \n",
        "  predicted_save_path=\"/content/drive/MyDrive/Farhana/DRAC2022_Prediction2/NEO0/\"\n",
        "\n",
        "  \"\"\" Hyperparameters \"\"\"\n",
        "  #checkpoint_path= \"/content/drive/MyDrive/Farhana/checkpoint/checkpoint_ima0.pth\"\n",
        "  #checkpoint_path= \"/content/drive/MyDrive/Farhana/checkpoint/checkpoint_npa0.pth\"\n",
        "  checkpoint_path= \"/content/drive/MyDrive/Farhana/checkpoint/checkpoint_neo0.pth\"\n",
        "  \n",
        "  \"\"\" Load the checkpoint \"\"\"\n",
        "  device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "  model = Model # IMA\n",
        "  \n",
        "  model = model.to(device)\n",
        "\n",
        "  #Loading the save model, Code:#model.load_state_dict(torch.load(PATH)),#model.eval()\n",
        "\n",
        "  model.load_state_dict(torch.load(checkpoint_path, map_location=device))\n",
        " \n",
        "  model.eval()\n",
        "  \n",
        "  predict_data( test_image_path, predicted_save_path)\n",
        " \n",
        "\n",
        "  print(\"\\nPrediction Completed\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B7u2dlxJVe3i"
      },
      "source": [
        "#Mask to nii"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dqqnaGSpPmXB",
        "outputId": "75ceb21b-c284-4ee5-b55a-c6a7ed13db4c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting SimpleITK\n",
            "  Downloading SimpleITK-2.2.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (52.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 52.8 MB 1.5 MB/s \n",
            "\u001b[?25hInstalling collected packages: SimpleITK\n",
            "Successfully installed SimpleITK-2.2.0\n"
          ]
        }
      ],
      "source": [
        "!pip install SimpleITK"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gaZ4nesrVkM9"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import cv2\n",
        "import SimpleITK\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def read_nii(nii_path, data_type=np.uint16):\n",
        "    img = SimpleITK.ReadImage(nii_path)\n",
        "    data = SimpleITK.GetArrayFromImage(img)\n",
        "    return np.array(data, dtype=data_type)\n",
        "\n",
        "\n",
        "def arr2nii(data, filename, reference_name=None):\n",
        "    img = SimpleITK.GetImageFromArray(data)\n",
        "    if (reference_name is not None):\n",
        "        img_ref = SimpleITK.ReadImage(reference_name)\n",
        "        img.CopyInformation(img_ref)\n",
        "    SimpleITK.WriteImage(img, filename)\n",
        "\n",
        "\n",
        "def masks2nii(mask_path):\n",
        "    mask_name_list = os.listdir(mask_path)\n",
        "    mask_name_list = sorted(mask_name_list, reverse=False, key=lambda x: int(x[:-4]))\n",
        "    mask_list = []\n",
        "    for mask_name in mask_name_list:\n",
        "        mask = cv2.imread(os.path.join(mask_path, mask_name), -1)\n",
        "        mask_list.append(mask)\n",
        "    arr2nii(np.array(mask_list, np.uint8), \"3.nii.gz\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    path = \"/content/drive/MyDrive/Farhana/DRAC2022_Prediction2/NEO0\"\n",
        "    masks2nii(path)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    \n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "toc_visible": true
    },
    "gpuClass": "premium",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "1c807654e6ee4329994b34c08b2d1aef": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "36f49d4a2c534e789b0b89569d9a9341": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1c807654e6ee4329994b34c08b2d1aef",
            "placeholder": "​",
            "style": "IPY_MODEL_8b45723e2fba45c1b333fc298d4af7ac",
            "value": "100%"
          }
        },
        "493a3e569a4a4a2f9209065dd5bda90c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "54fddd1d3210428a9cfd5cbfd469dd24": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "787ff4d662484357b9765d658d9f380b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_493a3e569a4a4a2f9209065dd5bda90c",
            "placeholder": "​",
            "style": "IPY_MODEL_abd31cadfa544556a629f92c4ebcf575",
            "value": " 161M/161M [00:00&lt;00:00, 246MB/s]"
          }
        },
        "80d9fb63d386430cb19f6a2072c58dac": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_36f49d4a2c534e789b0b89569d9a9341",
              "IPY_MODEL_9f8281320f2d4fc79ad3c16a6a293b22",
              "IPY_MODEL_787ff4d662484357b9765d658d9f380b"
            ],
            "layout": "IPY_MODEL_54fddd1d3210428a9cfd5cbfd469dd24"
          }
        },
        "8b45723e2fba45c1b333fc298d4af7ac": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9f8281320f2d4fc79ad3c16a6a293b22": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c516d91b49e2441782f0b0a5bde14c0b",
            "max": 168312152,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a5ccf5adf8ea4e26b431762a828d6663",
            "value": 168312152
          }
        },
        "a5ccf5adf8ea4e26b431762a828d6663": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "abd31cadfa544556a629f92c4ebcf575": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c516d91b49e2441782f0b0a5bde14c0b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}